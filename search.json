[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, Iâ€™m Onur Kerimoglu. Iâ€™m a data scientist with a backround in ecosystem modelling research, based in Hamburg, Germany (and thatâ€™s our pretty Alster behind me on my profile picture)."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my Quarto blog!\nHere I am planning to write about my ongoing projects, recipes, what worked and what not, in hopes that these will be helpful for someone out there, or my future self ðŸ˜ƒ\nEnjoy!\n\n\n\nImage credit: DreamStudio"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#detect-trend-and-seasonality",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#detect-trend-and-seasonality",
    "title": "Bayesian MMM",
    "section": "Detect Trend and Seasonality",
    "text": "Detect Trend and Seasonality\nLetâ€™s first detect the trend ans seasonility in the data, which we will use as control variables in our model.\n\nprophet_data = data_wdates.rename(columns = {'revenue': 'y', 'start_of_week': 'ds'})\n\nprophet = Prophet(yearly_seasonality=True, weekly_seasonality=False)\n\nprophet.fit(prophet_data[[\"ds\", \"y\"]])\nprophet_predict = prophet.predict(prophet_data[[\"ds\", \"y\"]])\n\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpsf7dam5e/4pfe8f3w.json\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpsf7dam5e/kumd0ckr.json\nDEBUG:cmdstanpy:idx 0\nDEBUG:cmdstanpy:running CmdStan, num_threads: None\nDEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=9274', 'data', 'file=/tmp/tmpsf7dam5e/4pfe8f3w.json', 'init=/tmp/tmpsf7dam5e/kumd0ckr.json', 'output', 'file=/tmp/tmpsf7dam5e/prophet_modeliit8_vzo/prophet_model-20230204201112.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n20:11:12 - cmdstanpy - INFO - Chain [1] start processing\nINFO:cmdstanpy:Chain [1] start processing\n20:11:12 - cmdstanpy - INFO - Chain [1] done processing\nINFO:cmdstanpy:Chain [1] done processing\n\n\n\nplot = prophet.plot_components(prophet_predict, figsize = (20, 10))\n\n\n\n\nAdding the detected trend and seasonality signals back to the data table:\n\nprophet_columns = [col for col in prophet_predict.columns if (col.endswith(\"upper\") == False) & (col.endswith(\"lower\") == False)]\n\nfinal_data = data_wdates.copy()\nfinal_data[\"trend\"] = prophet_predict[\"trend\"]\nfinal_data[\"season\"] = prophet_predict[\"yearly\"]\n\nThe final feature (X) and target (y) data:\n\nX = final_data.drop(columns=['revenue', 'start_of_week'])\ny = final_data['revenue']"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#carryover-adstock",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#carryover-adstock",
    "title": "Bayesian MMM",
    "section": "Carryover (adstock)",
    "text": "Carryover (adstock)\nFor modelling carryover, following Jin et al.Â 2017, we use an adstock function of form:\n\\[\nx^*_{t,m} = \\frac{\\sum_l w_m(l)x_{t-l,m}}{\\sum_l w_m(l)}\n\\]\nHere, \\(w_m\\) is a nonnegative weight function, which can be described with a geometric decay function, i.e.,\n\\[\nw^g_m = \\alpha_m^l,~l = 0, 1, ..., L-1,~0<\\alpha_m<1\n\\]\nwhere, \\(w_m\\) describes the weight of the effect on each time step \\(l\\), that lasts for \\(L\\) time steps (which we here prescribe to be 13 time steps, i.e., weeks, which is a good approximation for infinity according to Jin et al.Â 2017), and \\(\\alpha_m\\) is the decay rate for the channel \\(m\\).\nIn order to account for potential delays in the media effects, following Jin et al.Â 2017 again, we can add \\(\\theta_m\\), the delay of the peak effect for the media channel \\(m\\) into the equation above as follows:\n\\[\nw^d_m = \\alpha_m^{(l-\\theta_m)^2},~l = 0, 1, ..., L-1,~0<\\alpha_m<1,~0 \\leq \\theta_m \\leq L-1\n\\]\n\ndef adstock_weights(alpha, theta, length=13, delayed=False):\n    if delayed:\n        w = [tt.power(alpha, tt.power(i-theta,2)) for i in range(length)]\n    else:\n        w = [tt.power(alpha, i) for i in range(length)]\n    \n    w = tt.as_tensor_variable(w)\n    \n    return w\n\n\ndef carryover (x, alpha, theta=0, length=13, delayed=False):\n  \n    w = adstock_weights(alpha, theta, length, delayed)\n    \n    x_lags = tt.stack(\n        [tt.concatenate([tt.zeros(i),x[:x.shape[0]-i]]) for i in range(length)]\n    )\n    \n    return tt.dot(w, x_lags)\n\nNext, we build a model that consists of delayed media channels and control variables:\n\\[\n\\hat{y} = \\epsilon + \\tau +  \\sum_c \\gamma_c z_{t,c} + \\sum_m  \\beta_m x^*_{t,m}  \n\\]\nwhere, \\(\\epsilon\\) and \\(\\tau\\) represent noise and baseline revenue, \\(z_{t,c}\\) and \\(\\gamma\\) represent the control variable \\(c\\) and their effects, and \\(x^*_{t,m}\\) and \\(\\beta_m\\) represent the (adstocked) media spending \\(m\\) and their effects, respectively.\nNote that here for simplicity, we assume no shape effects (i.e., no saturation). We further assume that marketing contributions can only be positive, which can be achieved by drawing the contribution coefficient from a half-normal distribution.\n\ncontrol_variables = [\"trend\", \"season\"]\nmedia_channels = [f'spend_channel_{i}' for i in range(1,8)]\ntransform_variables = control_variables+media_channels\n\ny_transformed=y/10000 #rescale target variable\n\nX_transformed = X.copy() #Min-max scale the features\n\nnumerical_encoder_dict = {}\nfor feature in transform_variables:\n    scaler = MinMaxScaler()\n    original = final_data[feature].values.reshape(-1, 1)\n    transformed = scaler.fit_transform(original)\n    X_transformed[feature] = transformed\n    numerical_encoder_dict[feature] = scaler\n\nwith pm3.Model() as mmm1:\n\n    #baseline (tau) and noise\n    base = pm3.Normal(\"base\", np.mean(y_transformed.values), sigma = 2) #tau\n    #base = pm3.Exponential('base', lam=0.01)\n    noise = pm3.Exponential('noise', lam=0.1) #epsilon\n\n    #media effects\n    channel_contributions = []\n    for channel in media_channels:\n        print(f\"Media channels: Adding {channel}\")\n        x = X_transformed[channel].values\n        #channel coefficients (forced positive):\n        beta = pm3.HalfNormal(f'beta_{channel}', sigma = 2)\n        #adstock decay\n        alpha = pm3.Beta(f'alpha_{channel}', alpha=2, beta=2)\n        #delay\n        theta = pm3.Uniform(f'theta_{channel}',lower=0, upper=12)\n        x_star = carryover(\n                    x,\n                    alpha,\n                    theta,\n                    delayed=True\n                    )\n        channel_contribution = pm3.Deterministic(\n            f'contribution_{channel}',\n            beta * x_star,\n            )\n        channel_contributions.append(channel_contribution)\n    \n    #control effects\n    control_contributions = []\n    for control_var in control_variables:\n        print(f\"Control Variables: Adding {control_var}\")\n        z = X_transformed[control_var].values\n        #control variables\n        gamma = pm3.Normal(f\"gamma_{control_var}\", sigma = 3)\n        control_effect = gamma * z\n        control_contributions.append(control_effect)\n\n    #add everything\n    revenue = pm3.Normal(\n        'revenue',\n        mu= base + sum(control_contributions) + sum(channel_contributions),\n        sigma=noise,\n        observed=y_transformed\n    )\n\nMedia channels: Adding spend_channel_1\nMedia channels: Adding spend_channel_2\nMedia channels: Adding spend_channel_3\nMedia channels: Adding spend_channel_4\nMedia channels: Adding spend_channel_5\nMedia channels: Adding spend_channel_6\nMedia channels: Adding spend_channel_7\nControl Variables: Adding trend\nControl Variables: Adding season"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#do-the-prior-distributions-make-sense",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#do-the-prior-distributions-make-sense",
    "title": "Bayesian MMM",
    "section": "Do the prior distributions make sense?",
    "text": "Do the prior distributions make sense?\nWe can check whether the model estimates based on priors more or less make sense, as can be judged from a rough alignment of the corresponding estimates with the observations.\n\nwith mmm1:\n    prior_pred = pm3.sample_prior_predictive()\nprior_names = [prior_name for prior_name in list(prior_pred.keys()) if (prior_name.endswith(\"logodds__\") == False) & (prior_name.endswith(\"_log__\") == False)]\nfig, ax = plt.subplots(figsize = (20, 8))\n_ = ax.plot(prior_pred[\"revenue\"].T, color = \"0.5\", alpha = 0.1)\n_ = ax.plot(y_transformed.values, color = \"red\")\n\nWARNING:theano.tensor.blas:We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\nWARNING:theano.tensor.blas:We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n\n\n\n\n\nCheck the prior distributions:\n\n#plots priors using the random variables\ndef plot_priors(variables, prior_dictionary = None):\n    if isinstance(variables[0], pm3.model.TransformedRV) == False and prior_dictionary is None:\n        raise Exception(\"prior dictionary should be provided. It can be generated by sample_prior_predictive\")\n    cols = 7\n    rows = int(math.ceil(len(variables)/cols))\n    fig, ax = plt.subplots(rows, cols, figsize=(15, 3*rows))\n    ax = np.reshape(ax, (-1, cols))\n    for i in range(rows):\n         for j in range(cols):\n            vi = i*cols + j\n            if vi < len(variables):\n                var = variables[vi]\n                if isinstance(var, pm3.model.TransformedRV):\n                    sns.histplot(var.random(size=10000).flatten(), kde=True, ax=ax[i, j])\n                    #p.set_axis_labels(var.name)\n                    ax[i, j].set_title(var.name)\n                else:\n                    prior = prior_dictionary[var]\n                    sns.histplot(prior, kde=True, ax = ax[i, j])\n                    ax[i, j].set_title(var)\n    plt.tight_layout()\n\nmedia_coef_priors = [p for p in prior_names if p.startswith(\"beta\")]\nplot_priors(media_coef_priors, prior_pred)\nprint(f\"beta priors: {len(media_coef_priors)}\")\n\nadstock_priors = [p for p in prior_names if p.startswith(\"alpha\")]\nplot_priors(adstock_priors, prior_pred)\nprint(f\"alpha priors: {len(adstock_priors)}\")\n\nadstock_priors = [p for p in prior_names if p.startswith(\"theta\")]\nplot_priors(adstock_priors, prior_pred)\nprint(f\"theta priors: {len(adstock_priors)}\")\n\ncontrol_coef_priors = [p for p in prior_names if p.startswith(\"gamma_\")] + [\"base\", \"noise\"]\nplot_priors(control_coef_priors, prior_pred)\nprint(f\"gamma priors: {len(control_coef_priors)}\")\n\nbeta priors: 7\nalpha priors: 7\ntheta priors: 14\ngamma priors: 4"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#fit-the-model",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#fit-the-model",
    "title": "Bayesian MMM",
    "section": "Fit the model",
    "text": "Fit the model\n\nwith mmm1:\n  trace = pm3.sample(return_inferencedata=True, tune=3000, target_accept=0.95)\n  trace_summary = az.summary(trace)\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 02:44<00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 02:57<00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\n/usr/local/lib/python3.8/dist-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/usr/local/lib/python3.8/dist-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\ntrace_summary\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      base\n      7.155\n      1.224\n      4.886\n      9.464\n      0.033\n      0.023\n      1393.0\n      1355.0\n      1.00\n    \n    \n      gamma_trend\n      2.445\n      1.990\n      -1.465\n      5.856\n      0.056\n      0.040\n      1276.0\n      1215.0\n      1.00\n    \n    \n      gamma_season\n      4.993\n      1.951\n      1.466\n      8.759\n      0.055\n      0.039\n      1279.0\n      1303.0\n      1.00\n    \n    \n      noise\n      4.076\n      0.322\n      3.455\n      4.662\n      0.008\n      0.006\n      1713.0\n      1255.0\n      1.01\n    \n    \n      beta_spend_channel_1\n      1.556\n      1.210\n      0.000\n      3.724\n      0.049\n      0.034\n      514.0\n      681.0\n      1.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      contribution_spend_channel_7[99]\n      0.449\n      0.365\n      0.000\n      1.113\n      0.009\n      0.006\n      1412.0\n      953.0\n      1.00\n    \n    \n      contribution_spend_channel_7[100]\n      0.423\n      0.339\n      0.000\n      1.039\n      0.008\n      0.006\n      1512.0\n      936.0\n      1.00\n    \n    \n      contribution_spend_channel_7[101]\n      0.459\n      0.387\n      0.000\n      1.155\n      0.009\n      0.007\n      1447.0\n      992.0\n      1.00\n    \n    \n      contribution_spend_channel_7[102]\n      0.485\n      0.420\n      0.000\n      1.257\n      0.010\n      0.007\n      1356.0\n      952.0\n      1.00\n    \n    \n      contribution_spend_channel_7[103]\n      0.507\n      0.444\n      0.000\n      1.324\n      0.011\n      0.008\n      1303.0\n      953.0\n      1.00\n    \n  \n\n753 rows Ã— 9 columns"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#posterior-distributions",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#posterior-distributions",
    "title": "Bayesian MMM",
    "section": "Posterior distributions",
    "text": "Posterior distributions\n\naz.plot_posterior(\n    trace,\n    var_names=['~contribution'],\n    filter_vars='like'\n)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f9af5f12400>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af620fd00>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6554070>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6680850>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6a422e0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6afb130>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6d135b0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6bf6ca0>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f9af3bbbbb0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af3bc7280>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af47db640>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6f9e190>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6004550>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af5fe1730>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9afafc7730>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af176f490>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1789bb0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1731310>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1749a30>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af16f0190>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f9af17098b0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af16a5f10>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af16c97c0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1663eb0>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f9af16866a0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1622dc0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1653520>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9af15fec40>]],\n      dtype=object)"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#predictions-vs-observations",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#predictions-vs-observations",
    "title": "Bayesian MMM",
    "section": "Predictions vs Observations",
    "text": "Predictions vs Observations\nWe can now check the model skill by plotting the predictions and observations together, and calculating, e.g., MAE.\n\nwith mmm1:\n    posterior = pm3.sample_posterior_predictive(trace)\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 01:04<00:00]\n    \n    \n\n\n\ny_pred = posterior['revenue'].mean(0)*10000\ny_stds = posterior['revenue'].std(0)*10000\n\nMAE = mean_absolute_error(y.values, y_pred)\nMAPE = mean_absolute_percentage_error(y.values, y_pred)*100\nSkillStr = 'MAE: %5d\\nMAPE: %5.2f%%'%(MAE,MAPE)\nfig, ax = plt.subplots(figsize=(12, 6))\nplt.subplots_adjust(left=0.15,\n                        bottom=0.15,\n                        right=0.95,\n                        top=0.9)\nax.plot(y.values, linewidth=2, c='r', label='Observations')\nax.plot(y_pred, linewidth=1, c='b', label='Mean prediction')\nax.fill_between(np.arange(len(y)), y_pred - 2*y_stds, y_pred + 2*y_stds, alpha=0.33)\nax.text(0.85,0.9,SkillStr, transform=ax.transAxes)\nax.legend(loc='upper center')\nax.set_xlabel('Week')\nax.set_ylabel('Revenue')\nplt.show()\n\n\n\n\nExcept for two weeks, the observations lay within 2 standard deviations plus/minus the predictions. That instance is likely due to a special event, like a promotion or a holiday, which is not accounted for by the model. The mean absolute error corresponds to about 20% of the revenue."
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#channel-contributions-and-roi",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#channel-contributions-and-roi",
    "title": "Bayesian MMM",
    "section": "Channel Contributions and ROI",
    "text": "Channel Contributions and ROI\n\ndef compute_mean(trace, channel):\n    return (trace\n            .posterior[f'{channel}']\n            .values\n            .reshape(2000, 104)\n            .mean(0)\n           )\n\nchannels = [f'contribution_spend_channel_{i}' for i in range(1,8)]\n\nunadj_contributions = pd.DataFrame(\n    {'Base+Trend+Seas': trace.posterior['base'].values.mean()\n                 +trace.posterior['gamma_trend'].values.mean()\n                 +trace.posterior['gamma_season'].values.mean()},\n    index=X.index\n)\n\nfor channel in channels:\n    unadj_contributions[channel] = compute_mean(trace, channel)\n\nadj_contributions = (unadj_contributions\n                     .div(unadj_contributions.sum(axis=1), axis=0)\n                     .mul(y, axis=0)\n                    )\n\nax = (adj_contributions\n      .plot.area(\n          figsize=(12, 6),\n          linewidth=1,\n          title='Predicted Revenue and Breakdown',\n          ylabel='Revenue',\n          xlabel='Week'\n      )\n     )\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(\n    handles[::-1], labels[::-1],\n    title='Channels', loc=\"upper right\",\n    #bbox_to_anchor=(1.01, 0.5)\n)\nplt.show()\n\n\n\n\nAccording to this plot, the model suggests that a large portion of the revenue is not explained by marketing actions.\nFor each channel \\(m\\), percentage \\(ROI_m\\) can be calculated according to:\n\\(ROI_m = \\frac{\\sum_t C_{t,m} - \\sum_t S_{t,m}}{\\sum_t S_{t,m}} * 100\\)\nwhere \\(C_{t,m}\\) and \\(S_{t,m}\\) are the revenue contribution and spends to the media channel \\(m\\) at a given time step (\\(t\\)).\n\n#Calculate ROI for each channel\ntotal_contr = adj_contributions.sum(axis=0)\ntotal_spend = X.sum(axis=0)\n\nCchannels = [f'contribution_spend_channel_{i}' for i in range(1,8)]\n            \nSchannels = [f'spend_channel_{i}' for i in range(1,8)]\n\nROI_l= [None] * 7\nspend_l = [None] * 7\ncontr_l = [None] * 7\nfor i in range(7):\n    spend_l[i] = total_spend[Schannels[i]]\n    contr_l[i] = total_contr[Cchannels[i]]\n    ROI_l[i] = (contr_l[i] - spend_l[i])/spend_l[i] *100\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\nax1.bar(np.arange(1,8) - 0.2, spend_l, color = 'r', width = 0.4, label='Spend')\nax1.bar(np.arange(1,8) + 0.2, contr_l, color = 'b', width = 0.4, label='Contr')\nax1.set_xlabel('Marketing Channel')\nax1.set_ylabel('Contribution and Spends')\nax1.legend(loc='upper left')\n\nax2.bar(range(1,8),ROI_l)\nax2.set_xlabel('Marketing Channel')\nax2.set_ylabel('ROI (%)')\n\nplt.show()\n\n\n\n\nOur model suggests that only channels 1, 2 and 6 generate positive net gains. Among these channels, 2 seems most effective in terms of ROI, however, in terms of absolute revenue contribution, channel 6 is the most important source. Among the channels that results in net costs channel 7 is the one that requires most immediate attenion, both in terms of ROI and absolute net cost. Continued investment in channels 3 and 4 seem also questionable."
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#extract-trend-and-seasonality",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#extract-trend-and-seasonality",
    "title": "Bayesian MMM",
    "section": "Extract Trend and Seasonality",
    "text": "Extract Trend and Seasonality\n\nfrom prophet import Prophet\n\n\nprophet_data = data_wdates.rename(columns = {'revenue': 'y', 'start_of_week': 'ds'})\n\nprophet = Prophet(yearly_seasonality=True, weekly_seasonality=False)\n\nprophet.fit(prophet_data[[\"ds\", \"y\"]])\nprophet_predict = prophet.predict(prophet_data[[\"ds\", \"y\"]])\n\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpgikvg02q/hlf0pv09.json\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpgikvg02q/b5gcq0ux.json\nDEBUG:cmdstanpy:idx 0\nDEBUG:cmdstanpy:running CmdStan, num_threads: None\nDEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=10532', 'data', 'file=/tmp/tmpgikvg02q/hlf0pv09.json', 'init=/tmp/tmpgikvg02q/b5gcq0ux.json', 'output', 'file=/tmp/tmpgikvg02q/prophet_model68mgz_lw/prophet_model-20230126104611.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n10:46:11 - cmdstanpy - INFO - Chain [1] start processing\nINFO:cmdstanpy:Chain [1] start processing\n10:46:11 - cmdstanpy - INFO - Chain [1] done processing\nINFO:cmdstanpy:Chain [1] done processing\n\n\n\nplot = prophet.plot_components(prophet_predict, figsize = (20, 10))\n\n\n\n\n\nprophet_columns = [col for col in prophet_predict.columns if (col.endswith(\"upper\") == False) & (col.endswith(\"lower\") == False)]\nevents_numeric = prophet_predict[prophet_columns].filter(like = \"events_\").sum(axis = 1)\n\nfinal_data = data_wdates.copy()\nfinal_data[\"trend\"] = prophet_predict[\"trend\"]\nfinal_data[\"season\"] = prophet_predict[\"yearly\"]\n\n\nX = final_data.drop(columns=['revenue', 'start_of_week'])\ny = final_data['revenue']"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#do-the-prior-distributions-make-sense",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#do-the-prior-distributions-make-sense",
    "title": "Bayesian MMM",
    "section": "Do the prior distributions make sense?",
    "text": "Do the prior distributions make sense?\n\nwith mmm1:\n    prior_pred = pm3.sample_prior_predictive()\nprior_names = [prior_name for prior_name in list(prior_pred.keys()) if (prior_name.endswith(\"logodds__\") == False) & (prior_name.endswith(\"_log__\") == False)]\nfig, ax = plt.subplots(figsize = (20, 8))\n_ = ax.plot(prior_pred[\"sales\"].T, color = \"0.5\", alpha = 0.1)\n_ = ax.plot(y_transformed.values, color = \"red\")\n\nWARNING:theano.tensor.blas:We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\nWARNING:theano.tensor.blas:We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n\n\n\n\n\n\n#plots priors using the random variables\ndef plot_priors(variables, prior_dictionary = None):\n    if isinstance(variables[0], pm3.model.TransformedRV) == False and prior_dictionary is None:\n        raise Exception(\"prior dictionary should be provided. It can be generated by sample_prior_predictive\")\n    cols = 7\n    rows = int(math.ceil(len(variables)/cols))\n    fig, ax = plt.subplots(rows, cols, figsize=(15, 3*rows))\n    ax = np.reshape(ax, (-1, cols))\n    for i in range(rows):\n         for j in range(cols):\n            vi = i*cols + j\n            if vi < len(variables):\n                var = variables[vi]\n                if isinstance(var, pm3.model.TransformedRV):\n                    sns.histplot(var.random(size=10000).flatten(), kde=True, ax=ax[i, j])\n                    #p.set_axis_labels(var.name)\n                    ax[i, j].set_title(var.name)\n                else:\n                    prior = prior_dictionary[var]\n                    sns.histplot(prior, kde=True, ax = ax[i, j])\n                    ax[i, j].set_title(var)\n    plt.tight_layout()\n    \n\nadstock_priors = [p for p in prior_names if p.startswith(\"car\")]\nplot_priors(adstock_priors, prior_pred)\nprint(f\"carryover priors: {len(adstock_priors)}\")\n\n# alpha_priors = [p for p in prior_names if p.startswith(\"sat\")]\n# plot_priors(alpha_priors, prior_pred)\n# print(f\"sat priors: {len(alpha_priors)}\")\n\nmedia_coef_priors = [p for p in prior_names if p.startswith(\"coef\")]\nplot_priors(media_coef_priors, prior_pred)\nprint(f\"coef priors: {len(media_coef_priors)}\")\n\ncontrol_coef_priors = [p for p in prior_names if p.startswith(\"control_\")] + [\"base\"]\nplot_priors(control_coef_priors, prior_pred)\nprint(f\"control coef priors: {len(control_coef_priors)}\")\n\n#plot_priors([\"sigma\"], prior_pred)\n\nprint(f\"sigma prior: 1\")\n\ncarryover priors: 7\ncoef priors: 7\ncontrol coef priors: 3\nsigma prior: 1"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#fit-the-model",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#fit-the-model",
    "title": "Bayesian MMM",
    "section": "Fit the model",
    "text": "Fit the model\n\nwith mmm1:\n  trace = pm3.sample(return_inferencedata=True, tune=3000, target_accept=0.95)\n  trace_summary = az.summary(trace)\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 01:16<00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 01:15<00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\n/usr/local/lib/python3.8/dist-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\ntrace_summary\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      control_trend\n      2.226\n      2.019\n      -1.785\n      5.763\n      0.048\n      0.038\n      1738.0\n      1562.0\n      1.0\n    \n    \n      control_season\n      4.750\n      1.860\n      1.397\n      8.340\n      0.040\n      0.028\n      2176.0\n      1508.0\n      1.0\n    \n    \n      base\n      6.811\n      1.206\n      4.585\n      9.033\n      0.030\n      0.021\n      1646.0\n      1513.0\n      1.0\n    \n    \n      coef_spend_channel_1\n      0.915\n      0.777\n      0.005\n      2.268\n      0.017\n      0.012\n      1317.0\n      833.0\n      1.0\n    \n    \n      car_spend_channel_1\n      0.446\n      0.214\n      0.063\n      0.806\n      0.004\n      0.003\n      2634.0\n      1449.0\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      contribution_spend_channel_7[100]\n      0.481\n      0.323\n      0.000\n      1.034\n      0.008\n      0.006\n      1404.0\n      848.0\n      1.0\n    \n    \n      contribution_spend_channel_7[101]\n      0.635\n      0.414\n      0.000\n      1.346\n      0.010\n      0.007\n      1376.0\n      767.0\n      1.0\n    \n    \n      contribution_spend_channel_7[102]\n      0.651\n      0.424\n      0.001\n      1.382\n      0.010\n      0.007\n      1388.0\n      767.0\n      1.0\n    \n    \n      contribution_spend_channel_7[103]\n      0.701\n      0.456\n      0.000\n      1.486\n      0.011\n      0.008\n      1385.0\n      767.0\n      1.0\n    \n    \n      noise\n      3.919\n      0.306\n      3.376\n      4.498\n      0.007\n      0.005\n      1978.0\n      1310.0\n      1.0\n    \n  \n\n746 rows Ã— 9 columns"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#posterior-distributions",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#posterior-distributions",
    "title": "Bayesian MMM",
    "section": "Posterior distributions",
    "text": "Posterior distributions\n\naz.plot_posterior(\n    trace,\n    var_names=['~contribution'],\n    filter_vars='like'\n)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fabda7341f0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabda6135e0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabda3fd820>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fabda5197f0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabda2ba340>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabd96bab20>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9662e50>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabd96eee20>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9090a60>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fabd8ffd820>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabd7e2bf70>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9dd2a90>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9ccf670>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9cef880>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9c244f0>],\n       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9d9f4f0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9e3adf0>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fabdab3aa90>]],\n      dtype=object)"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#predictions-vs-observations",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#predictions-vs-observations",
    "title": "Bayesian MMM",
    "section": "Predictions vs Observations",
    "text": "Predictions vs Observations\n\nwith mmm1:\n    posterior = pm3.sample_posterior_predictive(trace)\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:28<00:00]\n    \n    \n\n\n\ny_pred = posterior['sales'].mean(0)*10000\ny_stds = posterior['sales'].std(0)*10000\n\nMAE = mean_absolute_error(y.values, y_pred)\nMAPE = mean_absolute_percentage_error(y.values, y_pred)*100\nSkillStr = 'MAE: %5d\\nMAPE: %5.2f%%'%(MAE,MAPE)\nfig, ax = plt.subplots(figsize=(12, 6))\nplt.subplots_adjust(left=0.15,\n                        bottom=0.15,\n                        right=0.95,\n                        top=0.9)\nax.plot(y.values, linewidth=2, c='r', label='Observations')\nax.plot(y_pred, linewidth=1, c='b', label='Mean prediction')\nax.fill_between(np.arange(len(y)), y_pred - 2*y_stds, y_pred + 2*y_stds, alpha=0.33)\nax.text(0.85,0.9,SkillStr, transform=ax.transAxes)\nax.legend(loc='upper center')\nax.set_xlabel('Week')\nax.set_ylabel('Revenue')\nplt.show()"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#channel-contributions-and-roi",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#channel-contributions-and-roi",
    "title": "Bayesian MMM",
    "section": "Channel Contributions and ROI",
    "text": "Channel Contributions and ROI\n\ndef compute_mean(trace, channel):\n    return (trace\n            .posterior[f'{channel}']\n            .values\n            .reshape(2000, 104)\n            .mean(0)\n           )\n\nchannels = [f'contribution_spend_channel_{i}' for i in range(1,8)]\n\nunadj_contributions = pd.DataFrame(\n    {'Base+Trend+Seas': trace.posterior['base'].values.mean()\n                 +trace.posterior['control_trend'].values.mean()\n                 +trace.posterior['control_season'].values.mean()},\n    index=X.index\n)\n\nfor channel in channels:\n    unadj_contributions[channel] = compute_mean(trace, channel)\n\nadj_contributions = (unadj_contributions\n                     .div(unadj_contributions.sum(axis=1), axis=0)\n                     .mul(y, axis=0)\n                    )\n\nax = (adj_contributions\n      .plot.area(\n          figsize=(12, 6),\n          linewidth=1,\n          title='Predicted Revenue and Breakdown',\n          ylabel='Revenue',\n          xlabel='Week'\n      )\n     )\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(\n    handles[::-1], labels[::-1],\n    title='Channels', loc=\"upper right\",\n    #bbox_to_anchor=(1.01, 0.5)\n)\nplt.show()\n\n\n\n\n\n#Calculate ROI for each channel\ntotal_contr = adj_contributions.sum(axis=0)\ntotal_spend = X.sum(axis=0)\n\nCchannels = [f'contribution_spend_channel_{i}' for i in range(1,8)]\n            \nSchannels = [f'spend_channel_{i}' for i in range(1,8)]\n\nROI_l= [None] * 7\nspend_l = [None] * 7\ncontr_l = [None] * 7\nfor i in range(7):\n    spend_l[i] = total_spend[Schannels[i]]\n    contr_l[i] = total_contr[Cchannels[i]]\n    ROI_l[i] = (contr_l[i] - spend_l[i])/spend_l[i] *100\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\nax1.bar(np.arange(1,8) - 0.2, spend_l, color = 'r', width = 0.4, label='Spend')\nax1.bar(np.arange(1,8) + 0.2, contr_l, color = 'b', width = 0.4, label='Contr')\nax1.set_xlabel('Marketing Channel')\nax1.set_ylabel('Contribution and Spends')\nax1.legend(loc='upper left')\n\nax2.bar(range(1,8),ROI_l)\nax2.set_xlabel('Marketing Channel')\nax2.set_ylabel('ROI (%)')\n\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OnurKerimoglu.github.io",
    "section": "",
    "text": "Bayesian MMM\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nOnur Kerimoglu\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nOnur Kerimoglu\n\n\n\n\n\n\nNo matching items"
  }
]