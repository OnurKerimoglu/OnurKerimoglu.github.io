[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OnurKerimoglu.github.io",
    "section": "",
    "text": "Fine-tuning an ImageNet model for Facial Expression Recognition\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nOnur Kerimoglu\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian MMM\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nOnur Kerimoglu\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nOnur Kerimoglu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#detect-trend-and-seasonality",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#detect-trend-and-seasonality",
    "title": "Bayesian MMM",
    "section": "Detect Trend and Seasonality",
    "text": "Detect Trend and Seasonality\nLetâ€™s first detect the trend ans seasonility in the data, which we will use as control variables in our model.\n\nprophet_data = data_wdates.rename(columns = {'revenue': 'y', 'start_of_week': 'ds'})\n\nprophet = Prophet(yearly_seasonality=True, weekly_seasonality=False)\n\nprophet.fit(prophet_data[[\"ds\", \"y\"]])\nprophet_predict = prophet.predict(prophet_data[[\"ds\", \"y\"]])\n\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpsf7dam5e/4pfe8f3w.json\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpsf7dam5e/kumd0ckr.json\nDEBUG:cmdstanpy:idx 0\nDEBUG:cmdstanpy:running CmdStan, num_threads: None\nDEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=9274', 'data', 'file=/tmp/tmpsf7dam5e/4pfe8f3w.json', 'init=/tmp/tmpsf7dam5e/kumd0ckr.json', 'output', 'file=/tmp/tmpsf7dam5e/prophet_modeliit8_vzo/prophet_model-20230204201112.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n20:11:12 - cmdstanpy - INFO - Chain [1] start processing\nINFO:cmdstanpy:Chain [1] start processing\n20:11:12 - cmdstanpy - INFO - Chain [1] done processing\nINFO:cmdstanpy:Chain [1] done processing\n\n\n\nplot = prophet.plot_components(prophet_predict, figsize = (20, 10))\n\n\n\n\n\n\n\n\nAdding the detected trend and seasonality signals back to the data table:\n\nprophet_columns = [col for col in prophet_predict.columns if (col.endswith(\"upper\") == False) & (col.endswith(\"lower\") == False)]\n\nfinal_data = data_wdates.copy()\nfinal_data[\"trend\"] = prophet_predict[\"trend\"]\nfinal_data[\"season\"] = prophet_predict[\"yearly\"]\n\nThe final feature (X) and target (y) data:\n\nX = final_data.drop(columns=['revenue', 'start_of_week'])\ny = final_data['revenue']"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#carryover-adstock",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#carryover-adstock",
    "title": "Bayesian MMM",
    "section": "Carryover (adstock)",
    "text": "Carryover (adstock)\nFor modelling carryover, following Jin et al.Â 2017, we use an adstock function of form:\n\\[\nx^*_{t,m} = \\frac{\\sum_l w_m(l)x_{t-l,m}}{\\sum_l w_m(l)}\n\\]\nHere, \\(w_m\\) is a nonnegative weight function, which can be described with a geometric decay function, i.e.,\n\\[\nw^g_m = \\alpha_m^l,~l = 0, 1, ..., L-1,~0&lt;\\alpha_m&lt;1\n\\]\nwhere, \\(w_m\\) describes the weight of the effect on each time step \\(l\\), that lasts for \\(L\\) time steps (which we here prescribe to be 13 time steps, i.e., weeks, which is a good approximation for infinity according to Jin et al.Â 2017), and \\(\\alpha_m\\) is the decay rate for the channel \\(m\\).\nIn order to account for potential delays in the media effects, following Jin et al.Â 2017 again, we can add \\(\\theta_m\\), the delay of the peak effect for the media channel \\(m\\) into the equation above as follows:\n\\[\nw^d_m = \\alpha_m^{(l-\\theta_m)^2},~l = 0, 1, ..., L-1,~0&lt;\\alpha_m&lt;1,~0 \\leq \\theta_m \\leq L-1\n\\]\n\ndef adstock_weights(alpha, theta, length=13, delayed=False):\n    if delayed:\n        w = [tt.power(alpha, tt.power(i-theta,2)) for i in range(length)]\n    else:\n        w = [tt.power(alpha, i) for i in range(length)]\n    \n    w = tt.as_tensor_variable(w)\n    \n    return w\n\n\ndef carryover (x, alpha, theta=0, length=13, delayed=False):\n  \n    w = adstock_weights(alpha, theta, length, delayed)\n    \n    x_lags = tt.stack(\n        [tt.concatenate([tt.zeros(i),x[:x.shape[0]-i]]) for i in range(length)]\n    )\n    \n    return tt.dot(w, x_lags)\n\nNext, we build a model that consists of delayed media channels and control variables:\n\\[\n\\hat{y} = \\epsilon + \\tau +  \\sum_c \\gamma_c z_{t,c} + \\sum_m  \\beta_m x^*_{t,m}  \n\\]\nwhere, \\(\\epsilon\\) and \\(\\tau\\) represent noise and baseline revenue, \\(z_{t,c}\\) and \\(\\gamma\\) represent the control variable \\(c\\) and their effects, and \\(x^*_{t,m}\\) and \\(\\beta_m\\) represent the (adstocked) media spending \\(m\\) and their effects, respectively.\nNote that here for simplicity, we assume no shape effects (i.e., no saturation). We further assume that marketing contributions can only be positive, which can be achieved by drawing the contribution coefficient from a half-normal distribution.\n\ncontrol_variables = [\"trend\", \"season\"]\nmedia_channels = [f'spend_channel_{i}' for i in range(1,8)]\ntransform_variables = control_variables+media_channels\n\ny_transformed=y/10000 #rescale target variable\n\nX_transformed = X.copy() #Min-max scale the features\n\nnumerical_encoder_dict = {}\nfor feature in transform_variables:\n    scaler = MinMaxScaler()\n    original = final_data[feature].values.reshape(-1, 1)\n    transformed = scaler.fit_transform(original)\n    X_transformed[feature] = transformed\n    numerical_encoder_dict[feature] = scaler\n\nwith pm3.Model() as mmm1:\n\n    #baseline (tau) and noise\n    base = pm3.Normal(\"base\", np.mean(y_transformed.values), sigma = 2) #tau\n    #base = pm3.Exponential('base', lam=0.01)\n    noise = pm3.Exponential('noise', lam=0.1) #epsilon\n\n    #media effects\n    channel_contributions = []\n    for channel in media_channels:\n        print(f\"Media channels: Adding {channel}\")\n        x = X_transformed[channel].values\n        #channel coefficients (forced positive):\n        beta = pm3.HalfNormal(f'beta_{channel}', sigma = 2)\n        #adstock decay\n        alpha = pm3.Beta(f'alpha_{channel}', alpha=2, beta=2)\n        #delay\n        theta = pm3.Uniform(f'theta_{channel}',lower=0, upper=12)\n        x_star = carryover(\n                    x,\n                    alpha,\n                    theta,\n                    delayed=True\n                    )\n        channel_contribution = pm3.Deterministic(\n            f'contribution_{channel}',\n            beta * x_star,\n            )\n        channel_contributions.append(channel_contribution)\n    \n    #control effects\n    control_contributions = []\n    for control_var in control_variables:\n        print(f\"Control Variables: Adding {control_var}\")\n        z = X_transformed[control_var].values\n        #control variables\n        gamma = pm3.Normal(f\"gamma_{control_var}\", sigma = 3)\n        control_effect = gamma * z\n        control_contributions.append(control_effect)\n\n    #add everything\n    revenue = pm3.Normal(\n        'revenue',\n        mu= base + sum(control_contributions) + sum(channel_contributions),\n        sigma=noise,\n        observed=y_transformed\n    )\n\nMedia channels: Adding spend_channel_1\nMedia channels: Adding spend_channel_2\nMedia channels: Adding spend_channel_3\nMedia channels: Adding spend_channel_4\nMedia channels: Adding spend_channel_5\nMedia channels: Adding spend_channel_6\nMedia channels: Adding spend_channel_7\nControl Variables: Adding trend\nControl Variables: Adding season"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#do-the-prior-distributions-make-sense",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#do-the-prior-distributions-make-sense",
    "title": "Bayesian MMM",
    "section": "Do the prior distributions make sense?",
    "text": "Do the prior distributions make sense?\nWe can check whether the model estimates based on priors more or less make sense, as can be judged from a rough alignment of the corresponding estimates with the observations.\n\nwith mmm1:\n    prior_pred = pm3.sample_prior_predictive()\nprior_names = [prior_name for prior_name in list(prior_pred.keys()) if (prior_name.endswith(\"logodds__\") == False) & (prior_name.endswith(\"_log__\") == False)]\nfig, ax = plt.subplots(figsize = (20, 8))\n_ = ax.plot(prior_pred[\"revenue\"].T, color = \"0.5\", alpha = 0.1)\n_ = ax.plot(y_transformed.values, color = \"red\")\n\nWARNING:theano.tensor.blas:We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\nWARNING:theano.tensor.blas:We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n\n\n\n\n\n\n\n\n\nCheck the prior distributions:\n\n#plots priors using the random variables\ndef plot_priors(variables, prior_dictionary = None):\n    if isinstance(variables[0], pm3.model.TransformedRV) == False and prior_dictionary is None:\n        raise Exception(\"prior dictionary should be provided. It can be generated by sample_prior_predictive\")\n    cols = 7\n    rows = int(math.ceil(len(variables)/cols))\n    fig, ax = plt.subplots(rows, cols, figsize=(15, 3*rows))\n    ax = np.reshape(ax, (-1, cols))\n    for i in range(rows):\n         for j in range(cols):\n            vi = i*cols + j\n            if vi &lt; len(variables):\n                var = variables[vi]\n                if isinstance(var, pm3.model.TransformedRV):\n                    sns.histplot(var.random(size=10000).flatten(), kde=True, ax=ax[i, j])\n                    #p.set_axis_labels(var.name)\n                    ax[i, j].set_title(var.name)\n                else:\n                    prior = prior_dictionary[var]\n                    sns.histplot(prior, kde=True, ax = ax[i, j])\n                    ax[i, j].set_title(var)\n    plt.tight_layout()\n\nmedia_coef_priors = [p for p in prior_names if p.startswith(\"beta\")]\nplot_priors(media_coef_priors, prior_pred)\nprint(f\"beta priors: {len(media_coef_priors)}\")\n\nadstock_priors = [p for p in prior_names if p.startswith(\"alpha\")]\nplot_priors(adstock_priors, prior_pred)\nprint(f\"alpha priors: {len(adstock_priors)}\")\n\nadstock_priors = [p for p in prior_names if p.startswith(\"theta\")]\nplot_priors(adstock_priors, prior_pred)\nprint(f\"theta priors: {len(adstock_priors)}\")\n\ncontrol_coef_priors = [p for p in prior_names if p.startswith(\"gamma_\")] + [\"base\", \"noise\"]\nplot_priors(control_coef_priors, prior_pred)\nprint(f\"gamma priors: {len(control_coef_priors)}\")\n\nbeta priors: 7\nalpha priors: 7\ntheta priors: 14\ngamma priors: 4"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#fit-the-model",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#fit-the-model",
    "title": "Bayesian MMM",
    "section": "Fit the model",
    "text": "Fit the model\n\nwith mmm1:\n  trace = pm3.sample(return_inferencedata=True, tune=3000, target_accept=0.95)\n  trace_summary = az.summary(trace)\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 02:44&lt;00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 02:57&lt;00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\n/usr/local/lib/python3.8/dist-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/usr/local/lib/python3.8/dist-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\ntrace_summary\n\n\n  \n    \n      \n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nbase\n7.155\n1.224\n4.886\n9.464\n0.033\n0.023\n1393.0\n1355.0\n1.00\n\n\ngamma_trend\n2.445\n1.990\n-1.465\n5.856\n0.056\n0.040\n1276.0\n1215.0\n1.00\n\n\ngamma_season\n4.993\n1.951\n1.466\n8.759\n0.055\n0.039\n1279.0\n1303.0\n1.00\n\n\nnoise\n4.076\n0.322\n3.455\n4.662\n0.008\n0.006\n1713.0\n1255.0\n1.01\n\n\nbeta_spend_channel_1\n1.556\n1.210\n0.000\n3.724\n0.049\n0.034\n514.0\n681.0\n1.00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\ncontribution_spend_channel_7[99]\n0.449\n0.365\n0.000\n1.113\n0.009\n0.006\n1412.0\n953.0\n1.00\n\n\ncontribution_spend_channel_7[100]\n0.423\n0.339\n0.000\n1.039\n0.008\n0.006\n1512.0\n936.0\n1.00\n\n\ncontribution_spend_channel_7[101]\n0.459\n0.387\n0.000\n1.155\n0.009\n0.007\n1447.0\n992.0\n1.00\n\n\ncontribution_spend_channel_7[102]\n0.485\n0.420\n0.000\n1.257\n0.010\n0.007\n1356.0\n952.0\n1.00\n\n\ncontribution_spend_channel_7[103]\n0.507\n0.444\n0.000\n1.324\n0.011\n0.008\n1303.0\n953.0\n1.00\n\n\n\n\n753 rows Ã— 9 columns"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#posterior-distributions",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#posterior-distributions",
    "title": "Bayesian MMM",
    "section": "Posterior distributions",
    "text": "Posterior distributions\n\naz.plot_posterior(\n    trace,\n    var_names=['~contribution'],\n    filter_vars='like'\n)\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af5f12400&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af620fd00&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6554070&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6680850&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6a422e0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6afb130&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6d135b0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6bf6ca0&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af3bbbbb0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af3bc7280&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af47db640&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6f9e190&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af6004550&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af5fe1730&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9afafc7730&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af176f490&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1789bb0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1731310&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1749a30&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af16f0190&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af17098b0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af16a5f10&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af16c97c0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1663eb0&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af16866a0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1622dc0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af1653520&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f9af15fec40&gt;]],\n      dtype=object)"
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#predictions-vs-observations",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#predictions-vs-observations",
    "title": "Bayesian MMM",
    "section": "Predictions vs Observations",
    "text": "Predictions vs Observations\nWe can now check the model skill by plotting the predictions and observations together, and calculating, e.g., MAE.\n\nwith mmm1:\n    posterior = pm3.sample_posterior_predictive(trace)\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 01:04&lt;00:00]\n    \n    \n\n\n\ny_pred = posterior['revenue'].mean(0)*10000\ny_stds = posterior['revenue'].std(0)*10000\n\nMAE = mean_absolute_error(y.values, y_pred)\nMAPE = mean_absolute_percentage_error(y.values, y_pred)*100\nSkillStr = 'MAE: %5d\\nMAPE: %5.2f%%'%(MAE,MAPE)\nfig, ax = plt.subplots(figsize=(12, 6))\nplt.subplots_adjust(left=0.15,\n                        bottom=0.15,\n                        right=0.95,\n                        top=0.9)\nax.plot(y.values, linewidth=2, c='r', label='Observations')\nax.plot(y_pred, linewidth=1, c='b', label='Mean prediction')\nax.fill_between(np.arange(len(y)), y_pred - 2*y_stds, y_pred + 2*y_stds, alpha=0.33)\nax.text(0.85,0.9,SkillStr, transform=ax.transAxes)\nax.legend(loc='upper center')\nax.set_xlabel('Week')\nax.set_ylabel('Revenue')\nplt.show()\n\n\n\n\n\n\n\n\nExcept for two weeks, the observations lay within 2 standard deviations plus/minus the predictions. That instance is likely due to a special event, like a promotion or a holiday, which is not accounted for by the model. The mean absolute error corresponds to about 20% of the revenue."
  },
  {
    "objectID": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#channel-contributions-and-roi",
    "href": "posts/bayesian_mmm/bayesian_mmm_example_enhanced_newdataset_quarto.html#channel-contributions-and-roi",
    "title": "Bayesian MMM",
    "section": "Channel Contributions and ROI",
    "text": "Channel Contributions and ROI\n\ndef compute_mean(trace, channel):\n    return (trace\n            .posterior[f'{channel}']\n            .values\n            .reshape(2000, 104)\n            .mean(0)\n           )\n\nchannels = [f'contribution_spend_channel_{i}' for i in range(1,8)]\n\nunadj_contributions = pd.DataFrame(\n    {'Base+Trend+Seas': trace.posterior['base'].values.mean()\n                 +trace.posterior['gamma_trend'].values.mean()\n                 +trace.posterior['gamma_season'].values.mean()},\n    index=X.index\n)\n\nfor channel in channels:\n    unadj_contributions[channel] = compute_mean(trace, channel)\n\nadj_contributions = (unadj_contributions\n                     .div(unadj_contributions.sum(axis=1), axis=0)\n                     .mul(y, axis=0)\n                    )\n\nax = (adj_contributions\n      .plot.area(\n          figsize=(12, 6),\n          linewidth=1,\n          title='Predicted Revenue and Breakdown',\n          ylabel='Revenue',\n          xlabel='Week'\n      )\n     )\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(\n    handles[::-1], labels[::-1],\n    title='Channels', loc=\"upper right\",\n    #bbox_to_anchor=(1.01, 0.5)\n)\nplt.show()\n\n\n\n\n\n\n\n\nAccording to this plot, the model suggests that a large portion of the revenue is not explained by marketing actions.\nFor each channel \\(m\\), percentage \\(ROI_m\\) can be calculated according to:\n\\(ROI_m = \\frac{\\sum_t C_{t,m} - \\sum_t S_{t,m}}{\\sum_t S_{t,m}} * 100\\)\nwhere \\(C_{t,m}\\) and \\(S_{t,m}\\) are the revenue contribution and spends to the media channel \\(m\\) at a given time step (\\(t\\)).\n\n#Calculate ROI for each channel\ntotal_contr = adj_contributions.sum(axis=0)\ntotal_spend = X.sum(axis=0)\n\nCchannels = [f'contribution_spend_channel_{i}' for i in range(1,8)]\n            \nSchannels = [f'spend_channel_{i}' for i in range(1,8)]\n\nROI_l= [None] * 7\nspend_l = [None] * 7\ncontr_l = [None] * 7\nfor i in range(7):\n    spend_l[i] = total_spend[Schannels[i]]\n    contr_l[i] = total_contr[Cchannels[i]]\n    ROI_l[i] = (contr_l[i] - spend_l[i])/spend_l[i] *100\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\nax1.bar(np.arange(1,8) - 0.2, spend_l, color = 'r', width = 0.4, label='Spend')\nax1.bar(np.arange(1,8) + 0.2, contr_l, color = 'b', width = 0.4, label='Contr')\nax1.set_xlabel('Marketing Channel')\nax1.set_ylabel('Contribution and Spends')\nax1.legend(loc='upper left')\n\nax2.bar(range(1,8),ROI_l)\nax2.set_xlabel('Marketing Channel')\nax2.set_ylabel('ROI (%)')\n\nplt.show()\n\n\n\n\n\n\n\n\nOur model suggests that only channels 1, 2 and 6 generate positive net gains. Among these channels, 2 seems most effective in terms of ROI, however, in terms of absolute revenue contribution, channel 6 is the most important source. Among the channels that results in net costs channel 7 is the one that requires most immediate attenion, both in terms of ROI and absolute net cost. Continued investment in channels 3 and 4 seem also questionable."
  },
  {
    "objectID": "posts/fer_model/facial-expression-recognition-transfer-learning.html",
    "href": "posts/fer_model/facial-expression-recognition-transfer-learning.html",
    "title": "Fine-tuning an ImageNet model for Facial Expression Recognition",
    "section": "",
    "text": "In this notebook, we address the facial expression recognition challenge with transfer learning.\nThe data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories:\n\n\n\ncategory\nemotion\n\n\n\n\n0\nAngry\n\n\n1\nDisgust\n\n\n2\nFear\n\n\n3\nHappy\n\n\n4\nSad\n\n\n5\nSurprise\n\n\n6\nNeutral\n\n\n\nDrCapa provided an excellent notebook, which presents a concise but nice data analysis, and a custom CNN model that achieves a testing accuracy of about 55%. Building on this work, I wanted to see if I can improve this by using pretrained MobileNet model, fine-tuning it, and including some data augmentation."
  },
  {
    "objectID": "posts/fer_model/facial-expression-recognition-transfer-learning.html#general-defintions-and-helper-functions",
    "href": "posts/fer_model/facial-expression-recognition-transfer-learning.html#general-defintions-and-helper-functions",
    "title": "Fine-tuning an ImageNet model for Facial Expression Recognition",
    "section": "General defintions and helper functions",
    "text": "General defintions and helper functions\n\n#Define callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.00008,\n    patience=11,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    min_delta=0.0001,\n    factor=0.25,\n    patience=4,\n    min_lr=1e-7,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]\n\n\n#General shape parameters\nIMG_SIZE = 48\nNUM_CLASSES = 7\nBATCH_SIZE = 64\n\n\n#A plotting function to visualize training progress\ndef render_history(history, suf=''):\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    plt.subplots_adjust(left=0.1,\n                        bottom=0.1,\n                        right=0.95,\n                        top=0.9,\n                        wspace=0.4)\n    \n    ax1.set_title(\"Losses\")\n    ax1.plot(history.history[\"loss\"], label=\"loss\")\n    ax1.plot(history.history[\"val_loss\"], label=\"val_loss\")\n    ax1.set_xlabel('epochs')\n    ax1.set_ylabel('value of the loss function')\n    ax1.legend()\n\n    ax2.set_title(\"Accuracies\")\n    ax2.plot(history.history[\"accuracy\"], label=\"accuracy\")\n    ax2.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n    ax2.set_xlabel('epochs')\n    ax2.set_ylabel('value of accuracy')\n    ax2.legend()\n    \n    plt.show()\n    suf = '' if suf == '' else '_'+suf\n    fig.savefig('loss_and_acc'+suf +'.png')"
  },
  {
    "objectID": "posts/fer_model/facial-expression-recognition-transfer-learning.html#model-construction",
    "href": "posts/fer_model/facial-expression-recognition-transfer-learning.html#model-construction",
    "title": "Fine-tuning an ImageNet model for Facial Expression Recognition",
    "section": "Model construction",
    "text": "Model construction\n\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.models import Model\n\n#By specifying the include_top=False argument, we load a network that \n#doesn't include the  classification layers at the top, which is ideal for feature extraction.\nbase_net = MobileNet(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                     include_top=False,\n                     weights='imagenet')\n\n#plot_model(base_net, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=50, to_file='mobilenet_full.png')\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n17227776/17225924 [==============================] - 0s 0us/step\n\n\nFor these small images, mobilenet is a very large model. Observing that there is nothing left to convolve further, we take the model only until the 12.block\n\nbase_model = Model(inputs = base_net.input,outputs = base_net.get_layer('conv_pw_12_relu').output, name = 'mobilenet_trunc')\n#this is the same as:\n#base_model = Model(inputs = base_net.input,outputs = base_net.layers[-7].output)\n\n#plot_model(base_model, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=50, to_file='mobilenet_truncated.png')\n\n\n#from: https://www.tensorflow.org/tutorials/images/transfer_learning\nfrom tensorflow.keras import Sequential, layers\nfrom tensorflow.keras import Input, Model\n#from tensor\n\n#base_model.trainable = False\n\n#This model expects pixel values in [-1, 1], but at this point, the pixel values in your images are in [0, 255]. \n#To rescale them, use the preprocessing method included with the model.\n#preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n\n#Add a classification head: To generate predictions from the block of features, \n#average over the spatial 2x2 spatial locations,  using a tf.keras.layers.GlobalAveragePooling2D layer \n#to convert the features to a single 1280-element vector per image.\nglobal_average_layer = GlobalAvgPool2D()\n#feature_batch_average = global_average_layer(feature_batch)\n#print(feature_batch_average.shape)\n\n#Apply a tf.keras.layers.Dense layer to convert these features into a single prediction per image. \n#You don't need an activation function here because this prediction will be treated as a logit, \n#or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0.\nprediction_layer = Dense(NUM_CLASSES, activation=\"softmax\", name=\"pred\")\n#prediction_batch = prediction_layer(feature_batch_average)\n#print(prediction_batch.shape)\n\n#Build a model by chaining together the data augmentation, rescaling, base_model and feature extractor layers \n#using the Keras Functional API. As previously mentioned, use training=False as our model contains a BatchNormalization layer.\ninputs_raw = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n#inputs_pp = preprocess_input(inputs_aug)\n#x = base_model(inputs_pp, training=False)\nx = base_model(inputs_raw, training=False)\nx = global_average_layer(x)\n#x = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = Model(inputs=inputs_raw, outputs= outputs)\n\nmodel.summary()\nplot_model(model, \n           show_shapes=True, \n           show_layer_names=True, \n           expand_nested=True, \n           dpi=50, \n           to_file='MobileNet12blocks_structure.png')\n\nModel: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 48, 48, 3)]       0         \n_________________________________________________________________\nmobilenet_trunc (Functional) (None, 1, 1, 1024)        2162880   \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1024)              0         \n_________________________________________________________________\npred (Dense)                 (None, 7)                 7175      \n=================================================================\nTotal params: 2,170,055\nTrainable params: 2,152,263\nNon-trainable params: 17,792\n_________________________________________________________________\n\n\n\n\n\n\n\n\n\n\n#Train the classification head:\n\n#base_model.trainable = True #if we included the model layers, but not the model itself, this doesn't have any effect\nfor layer in base_model.layers[:]:\n    layer.trainable = False\n#for layer in base_model.layers[81:]:\n#    layer.trainable = True\n\noptims = {\n    'sgd': optimizers.SGD(lr=0.1, momentum=0.9, decay=0.01),\n    'adam': optimizers.Adam(0.01),\n    'nadam': optimizers.Nadam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n}\n\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optims['adam'],\n        metrics=['accuracy']\n)\n\nmodel.summary()\n\nModel: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 48, 48, 3)]       0         \n_________________________________________________________________\nmobilenet_trunc (Functional) (None, 1, 1, 1024)        2162880   \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1024)              0         \n_________________________________________________________________\npred (Dense)                 (None, 7)                 7175      \n=================================================================\nTotal params: 2,170,055\nTrainable params: 7,175\nNon-trainable params: 2,162,880\n_________________________________________________________________\n\n\n\ninitial_epochs = 5\n# total_epochs = initial_epochs + 5\nhistory = model.fit_generator(train_rgb_datagen.flow(train_images_rgb, \n                                                 train_labels, \n                                                 batch_size=BATCH_SIZE),\n                              validation_data=(val_images_rgb, \n                                               val_labels),\n                              class_weight = class_weight,\n                              steps_per_epoch=len(train_images) / BATCH_SIZE,\n                              #initial_epoch = history.epoch[-1],\n                              #epochs = total_epochs,\n                              epochs = initial_epochs,\n                              callbacks=callbacks,\n                              use_multiprocessing=True)\n\nEpoch 1/5\n449/448 [==============================] - 23s 51ms/step - loss: 0.3093 - accuracy: 0.3429 - val_loss: 1.8735 - val_accuracy: 0.3892\nEpoch 2/5\n449/448 [==============================] - 22s 49ms/step - loss: 0.2979 - accuracy: 0.3632 - val_loss: 1.9810 - val_accuracy: 0.3461\nEpoch 3/5\n449/448 [==============================] - 21s 47ms/step - loss: 0.2963 - accuracy: 0.3704 - val_loss: 1.9980 - val_accuracy: 0.4018\nEpoch 4/5\n449/448 [==============================] - 22s 50ms/step - loss: 0.3032 - accuracy: 0.3677 - val_loss: 2.1077 - val_accuracy: 0.3770\nEpoch 5/5\n449/448 [==============================] - 22s 49ms/step - loss: 0.2978 - accuracy: 0.3697 - val_loss: 1.8218 - val_accuracy: 0.3954\n\n\n\nFine-tuning\nHere I wanted to find out whether the training converges better or faster if the training is performed iteratively, whereby first the upper layers of the base_model is fine-tuned with a moderately slow learning rate (1e-3), then the entire base model will be fine-tuned in a second round with a small learning rate (1e-4). In the non-iterative approach, the whole base_model is trained with that smal learning rate (1e-4). However I did not see evidence for any advantage of the iterative approach, therefore Iâ€™ve set the â€˜iterative_finetuningâ€™ switch to False.\n\niterative_finetuning = False \n\n\nFirst iteration: partial fine-tuning of the base_model\n\nif iterative_finetuning:\n    #fine-tune the top layers (blocks 7-12):\n\n    # Let's take a look to see how many layers are in the base model\n    print(\"Number of layers in the base model: \", len(base_model.layers))\n    #base_model.trainable = True #if we included the model layers, but not the model itself, this doesn't have any effect\n    for layer in base_model.layers:\n        layer.trainable = False\n    for layer in base_model.layers[-37:]: #blocks 7-12\n      layer.trainable = True\n\n    optims = {\n        'sgd': optimizers.SGD(lr=0.01, momentum=0.9, decay=0.01),\n        'adam': optimizers.Adam(0.001),\n        'nadam': optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n    }\n\n    model.compile(\n            loss='categorical_crossentropy',\n            optimizer=optims['adam'],\n            metrics=['accuracy']\n    )\n\n    model.summary()\n\n\nif iterative_finetuning:\n    fine_tune_epochs = 40\n    total_epochs =  history.epoch[-1] + fine_tune_epochs\n\n    history = model.fit_generator(train_rgb_datagen.flow(train_images_rgb, \n                                                     train_labels, \n                                                     batch_size=BATCH_SIZE),\n                                  validation_data=(val_images_rgb, \n                                                   val_labels),\n                                  class_weight = class_weight,\n                                  steps_per_epoch=len(train_images) / BATCH_SIZE,\n                                  initial_epoch = history.epoch[-1],\n                                  epochs = total_epochs,\n                                  callbacks=callbacks,\n                                  use_multiprocessing=True)\n\n\nif iterative_finetuning:\n    test_loss, test_acc = model.evaluate(test_images_rgb, test_labels) #, test_labels\n    print('test caccuracy:', test_acc)\n\n\nif iterative_finetuning:\n    render_history(history, 'mobilenet12blocks_wdgenaug_finetuning1')\n\n\n\nSecond Iteration (or the main iteration, if iterative_finetuning was set to False): fine-tuning of the entire base_model\n\nif iterative_finetuning:\n    ftsuf = 'ft_2'\nelse:\n    ftsuf = 'ft_atonce'\n\n\n#fine-tune all layers\n\n# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(base_model.layers))\n#base_model.trainable = True #if we included the model layers, but not the model itself, this doesn't have any effect\nfor layer in base_model.layers:\n    layer.trainable = False\nfor layer in base_model.layers[:]:\n  layer.trainable = True\n\noptims = {\n    'sgd': optimizers.SGD(lr=0.01, momentum=0.9, decay=0.01),\n    'adam': optimizers.Adam(0.0001),\n    'nadam': optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n}\n\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optims['adam'],\n        metrics=['accuracy']\n)\n\nmodel.summary()\n\nNumber of layers in the base model:  81\nModel: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 48, 48, 3)]       0         \n_________________________________________________________________\nmobilenet_trunc (Functional) (None, 1, 1, 1024)        2162880   \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1024)              0         \n_________________________________________________________________\npred (Dense)                 (None, 7)                 7175      \n=================================================================\nTotal params: 2,170,055\nTrainable params: 2,152,263\nNon-trainable params: 17,792\n_________________________________________________________________\n\n\n\nfine_tune_epochs = 100\ntotal_epochs =  history.epoch[-1] + fine_tune_epochs\n\nhistory = model.fit_generator(train_rgb_datagen.flow(train_images_rgb, \n                                                 train_labels, \n                                                 batch_size=BATCH_SIZE),\n                              validation_data=(val_images_rgb, \n                                               val_labels),\n                              class_weight = class_weight,\n                              steps_per_epoch=len(train_images) / BATCH_SIZE,\n                              initial_epoch = history.epoch[-1],\n                              epochs = total_epochs,\n                              callbacks=callbacks,\n                              use_multiprocessing=True)\n\nEpoch 5/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.2373 - accuracy: 0.4185 - val_loss: 1.4505 - val_accuracy: 0.4728\nEpoch 6/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.2106 - accuracy: 0.4788 - val_loss: 1.3257 - val_accuracy: 0.5096\nEpoch 7/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.1995 - accuracy: 0.5043 - val_loss: 1.2294 - val_accuracy: 0.5411\nEpoch 8/104\n449/448 [==============================] - 25s 55ms/step - loss: 0.1947 - accuracy: 0.5164 - val_loss: 1.2620 - val_accuracy: 0.5272\nEpoch 9/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.1886 - accuracy: 0.5320 - val_loss: 1.2909 - val_accuracy: 0.5032\nEpoch 10/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.1853 - accuracy: 0.5416 - val_loss: 1.1900 - val_accuracy: 0.5634\nEpoch 11/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.1797 - accuracy: 0.5533 - val_loss: 1.1744 - val_accuracy: 0.5717\nEpoch 12/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.1780 - accuracy: 0.5587 - val_loss: 1.2182 - val_accuracy: 0.5425\nEpoch 13/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.1744 - accuracy: 0.5685 - val_loss: 1.1660 - val_accuracy: 0.5665\nEpoch 14/104\n449/448 [==============================] - 25s 55ms/step - loss: 0.1719 - accuracy: 0.5716 - val_loss: 1.1556 - val_accuracy: 0.5740\nEpoch 15/104\n449/448 [==============================] - 25s 55ms/step - loss: 0.1707 - accuracy: 0.5766 - val_loss: 1.1153 - val_accuracy: 0.5924\nEpoch 16/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.1676 - accuracy: 0.5822 - val_loss: 1.1014 - val_accuracy: 0.5890\nEpoch 17/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.1658 - accuracy: 0.5891 - val_loss: 1.0770 - val_accuracy: 0.6088\nEpoch 18/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.1645 - accuracy: 0.5895 - val_loss: 1.0803 - val_accuracy: 0.6013\nEpoch 19/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.1615 - accuracy: 0.5985 - val_loss: 1.0768 - val_accuracy: 0.5952\nEpoch 20/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.1602 - accuracy: 0.6019 - val_loss: 1.0971 - val_accuracy: 0.5932\nEpoch 21/104\n449/448 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.6041\nEpoch 00021: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n449/448 [==============================] - 26s 57ms/step - loss: 0.1591 - accuracy: 0.6041 - val_loss: 1.0884 - val_accuracy: 0.6007\nEpoch 22/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.1487 - accuracy: 0.6299 - val_loss: 1.0391 - val_accuracy: 0.6158\nEpoch 23/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.1465 - accuracy: 0.6351 - val_loss: 1.0547 - val_accuracy: 0.6158\nEpoch 24/104\n449/448 [==============================] - 26s 57ms/step - loss: 0.1455 - accuracy: 0.6372 - val_loss: 1.0437 - val_accuracy: 0.6141\nEpoch 25/104\n449/448 [==============================] - 26s 57ms/step - loss: 0.1449 - accuracy: 0.6379 - val_loss: 1.0374 - val_accuracy: 0.6236\nEpoch 26/104\n449/448 [==============================] - 25s 57ms/step - loss: 0.1441 - accuracy: 0.6361 - val_loss: 1.0269 - val_accuracy: 0.6216\nEpoch 27/104\n449/448 [==============================] - 24s 53ms/step - loss: 0.1426 - accuracy: 0.6414 - val_loss: 1.0445 - val_accuracy: 0.6186\nEpoch 28/104\n449/448 [==============================] - 26s 58ms/step - loss: 0.1431 - accuracy: 0.6438 - val_loss: 1.0358 - val_accuracy: 0.6213\nEpoch 29/104\n449/448 [==============================] - 26s 57ms/step - loss: 0.1406 - accuracy: 0.6493 - val_loss: 1.0244 - val_accuracy: 0.6239\nEpoch 30/104\n449/448 [==============================] - 26s 59ms/step - loss: 0.1409 - accuracy: 0.6473 - val_loss: 1.0648 - val_accuracy: 0.6124\nEpoch 31/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.1405 - accuracy: 0.6517 - val_loss: 1.0506 - val_accuracy: 0.6158\nEpoch 32/104\n449/448 [==============================] - 26s 57ms/step - loss: 0.1392 - accuracy: 0.6518 - val_loss: 1.0222 - val_accuracy: 0.6208\nEpoch 33/104\n448/448 [============================&gt;.] - ETA: 0s - loss: 0.1391 - accuracy: 0.6556\nEpoch 00033: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n449/448 [==============================] - 27s 59ms/step - loss: 0.1391 - accuracy: 0.6554 - val_loss: 1.0329 - val_accuracy: 0.6208\nEpoch 34/104\n449/448 [==============================] - 26s 58ms/step - loss: 0.1348 - accuracy: 0.6613 - val_loss: 1.0199 - val_accuracy: 0.6255\nEpoch 35/104\n449/448 [==============================] - 25s 55ms/step - loss: 0.1338 - accuracy: 0.6652 - val_loss: 1.0257 - val_accuracy: 0.6280\nEpoch 36/104\n449/448 [==============================] - 27s 59ms/step - loss: 0.1326 - accuracy: 0.6690 - val_loss: 1.0187 - val_accuracy: 0.6219\nEpoch 37/104\n449/448 [==============================] - 26s 58ms/step - loss: 0.1335 - accuracy: 0.6651 - val_loss: 1.0161 - val_accuracy: 0.6297\nEpoch 38/104\n449/448 [==============================] - 23s 52ms/step - loss: 0.1330 - accuracy: 0.6678 - val_loss: 1.0254 - val_accuracy: 0.6252\nEpoch 39/104\n449/448 [==============================] - 27s 59ms/step - loss: 0.1326 - accuracy: 0.6685 - val_loss: 1.0133 - val_accuracy: 0.6317\nEpoch 40/104\n449/448 [==============================] - 27s 59ms/step - loss: 0.1327 - accuracy: 0.6673 - val_loss: 1.0138 - val_accuracy: 0.6330\nEpoch 41/104\n449/448 [==============================] - 27s 60ms/step - loss: 0.1323 - accuracy: 0.6703 - val_loss: 1.0157 - val_accuracy: 0.6261\nEpoch 42/104\n449/448 [==============================] - 25s 56ms/step - loss: 0.1330 - accuracy: 0.6685 - val_loss: 1.0189 - val_accuracy: 0.6289\nEpoch 43/104\n449/448 [==============================] - 26s 58ms/step - loss: 0.1326 - accuracy: 0.6675 - val_loss: 1.0165 - val_accuracy: 0.6283\nEpoch 44/104\n448/448 [============================&gt;.] - ETA: 0s - loss: 0.1317 - accuracy: 0.6701\nEpoch 00044: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n449/448 [==============================] - 27s 60ms/step - loss: 0.1317 - accuracy: 0.6701 - val_loss: 1.0146 - val_accuracy: 0.6280\nEpoch 45/104\n449/448 [==============================] - 24s 54ms/step - loss: 0.1308 - accuracy: 0.6737 - val_loss: 1.0165 - val_accuracy: 0.6300\nEpoch 46/104\n449/448 [==============================] - 27s 61ms/step - loss: 0.1309 - accuracy: 0.6746 - val_loss: 1.0142 - val_accuracy: 0.6317\nEpoch 47/104\n449/448 [==============================] - 27s 61ms/step - loss: 0.1297 - accuracy: 0.6779 - val_loss: 1.0161 - val_accuracy: 0.6278\nEpoch 48/104\n449/448 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.6748\nEpoch 00048: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n449/448 [==============================] - 24s 54ms/step - loss: 0.1298 - accuracy: 0.6748 - val_loss: 1.0151 - val_accuracy: 0.6328\nEpoch 49/104\n449/448 [==============================] - 24s 53ms/step - loss: 0.1296 - accuracy: 0.6771 - val_loss: 1.0138 - val_accuracy: 0.6328\nEpoch 50/104\n449/448 [==============================] - 28s 62ms/step - loss: 0.1306 - accuracy: 0.6742 - val_loss: 1.0144 - val_accuracy: 0.6328\nEpoch 51/104\n448/448 [============================&gt;.] - ETA: 0s - loss: 0.1293 - accuracy: 0.6780Restoring model weights from the end of the best epoch.\n449/448 [==============================] - 24s 54ms/step - loss: 0.1293 - accuracy: 0.6780 - val_loss: 1.0146 - val_accuracy: 0.6328\nEpoch 00051: early stopping\n\n\n\ntest_loss, test_acc = model.evaluate(test_images_rgb, test_labels) #, test_labels\nprint('test caccuracy:', test_acc)\n\n113/113 [==============================] - 0s 4ms/step - loss: 1.0364 - accuracy: 0.6236\ntest caccuracy: 0.623572051525116\n\n\n\nrender_history(history, 'mobilenet12blocks_wdgenaug_'+ftsuf)\n\n\n\n\n\n\n\n\n\npred_test_labels = model.predict(test_images_rgb)\n\n\nmodel_yaml = model.to_yaml()\nwith open('MobileNet12blocks_wdgenaug_onrawdata_valacc_' + ftsuf + '.yaml', 'w') as yaml_file:\n    yaml_file.write(model_yaml)\n    \nmodel.save('MobileNet12blocks_wdgenaug_onrawdata_valacc_' + ftsuf + '.h5')"
  },
  {
    "objectID": "posts/fer_model/facial-expression-recognition-transfer-learning.html#compare-the-distribution-of-labels-and-predicted-labels",
    "href": "posts/fer_model/facial-expression-recognition-transfer-learning.html#compare-the-distribution-of-labels-and-predicted-labels",
    "title": "Fine-tuning an ImageNet model for Facial Expression Recognition",
    "section": "Compare the distribution of labels and predicted labels",
    "text": "Compare the distribution of labels and predicted labels\n\ndef plot_compare_distributions(array1, array2, title1='', title2=''):\n    df_array1 = pd.DataFrame()\n    df_array2 = pd.DataFrame()\n    df_array1['emotion'] = array1.argmax(axis=1)\n    df_array2['emotion'] = array2.argmax(axis=1)\n    \n    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n    x = emotions.values()\n    \n    y = df_array1['emotion'].value_counts()\n    keys_missed = list(set(emotions.keys()).difference(set(y.keys())))\n    for key_missed in keys_missed:\n        y[key_missed] = 0\n    axs[0].bar(x, y.sort_index(), color='orange')\n    axs[0].set_title(title1)\n    axs[0].grid()\n    \n    y = df_array2['emotion'].value_counts()\n    keys_missed = list(set(emotions.keys()).difference(set(y.keys())))\n    for key_missed in keys_missed:\n        y[key_missed] = 0\n    axs[1].bar(x, y.sort_index())\n    axs[1].set_title(title2)\n    axs[1].grid()\n    \n    plt.show()\n\n\nplot_compare_distributions(test_labels, pred_test_labels, title1='test labels', title2='predict labels')"
  },
  {
    "objectID": "posts/fer_model/facial-expression-recognition-transfer-learning.html#wrong-predictions",
    "href": "posts/fer_model/facial-expression-recognition-transfer-learning.html#wrong-predictions",
    "title": "Fine-tuning an ImageNet model for Facial Expression Recognition",
    "section": "Wrong Predictions",
    "text": "Wrong Predictions\nThe accuracy score is about 63% on the test set. Letâ€™s try to understand where the model is doing wrong.\n\ndf_compare = pd.DataFrame()\ndf_compare['real'] = test_labels.argmax(axis=1)\ndf_compare['pred'] = pred_test_labels.argmax(axis=1)\ndf_compare['wrong'] = np.where(df_compare['real']!=df_compare['pred'], 1, 0)\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nconf_mat = confusion_matrix(test_labels.argmax(axis=1), pred_test_labels.argmax(axis=1))\n\nfig, ax = plot_confusion_matrix(conf_mat=conf_mat,\n                                show_normed=True,\n                                show_absolute=False,\n                                class_names=emotions.values(),\n                                figsize=(8, 8))\nfig.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, Iâ€™m Onur Kerimoglu. Iâ€™m a data scientist with a backround in ecosystem modelling research, based in Hamburg, Germany (and thatâ€™s our pretty Alster behind me on my profile picture)."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my Quarto blog!\nHere I am planning to write about my ongoing projects, recipes, what worked and what not, in hopes that these will be helpful for someone out there, or my future self ðŸ˜ƒ\nEnjoy!\n\n\n\nImage credit: DreamStudio"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#extract-trend-and-seasonality",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#extract-trend-and-seasonality",
    "title": "Bayesian MMM",
    "section": "Extract Trend and Seasonality",
    "text": "Extract Trend and Seasonality\n\nfrom prophet import Prophet\n\n\nprophet_data = data_wdates.rename(columns = {'revenue': 'y', 'start_of_week': 'ds'})\n\nprophet = Prophet(yearly_seasonality=True, weekly_seasonality=False)\n\nprophet.fit(prophet_data[[\"ds\", \"y\"]])\nprophet_predict = prophet.predict(prophet_data[[\"ds\", \"y\"]])\n\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpgikvg02q/hlf0pv09.json\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpgikvg02q/b5gcq0ux.json\nDEBUG:cmdstanpy:idx 0\nDEBUG:cmdstanpy:running CmdStan, num_threads: None\nDEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.8/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=10532', 'data', 'file=/tmp/tmpgikvg02q/hlf0pv09.json', 'init=/tmp/tmpgikvg02q/b5gcq0ux.json', 'output', 'file=/tmp/tmpgikvg02q/prophet_model68mgz_lw/prophet_model-20230126104611.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n10:46:11 - cmdstanpy - INFO - Chain [1] start processing\nINFO:cmdstanpy:Chain [1] start processing\n10:46:11 - cmdstanpy - INFO - Chain [1] done processing\nINFO:cmdstanpy:Chain [1] done processing\n\n\n\nplot = prophet.plot_components(prophet_predict, figsize = (20, 10))\n\n\n\n\n\n\n\n\n\nprophet_columns = [col for col in prophet_predict.columns if (col.endswith(\"upper\") == False) & (col.endswith(\"lower\") == False)]\nevents_numeric = prophet_predict[prophet_columns].filter(like = \"events_\").sum(axis = 1)\n\nfinal_data = data_wdates.copy()\nfinal_data[\"trend\"] = prophet_predict[\"trend\"]\nfinal_data[\"season\"] = prophet_predict[\"yearly\"]\n\n\nX = final_data.drop(columns=['revenue', 'start_of_week'])\ny = final_data['revenue']"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#do-the-prior-distributions-make-sense",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#do-the-prior-distributions-make-sense",
    "title": "Bayesian MMM",
    "section": "Do the prior distributions make sense?",
    "text": "Do the prior distributions make sense?\n\nwith mmm1:\n    prior_pred = pm3.sample_prior_predictive()\nprior_names = [prior_name for prior_name in list(prior_pred.keys()) if (prior_name.endswith(\"logodds__\") == False) & (prior_name.endswith(\"_log__\") == False)]\nfig, ax = plt.subplots(figsize = (20, 8))\n_ = ax.plot(prior_pred[\"sales\"].T, color = \"0.5\", alpha = 0.1)\n_ = ax.plot(y_transformed.values, color = \"red\")\n\nWARNING:theano.tensor.blas:We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\nWARNING:theano.tensor.blas:We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n\n\n\n\n\n\n\n\n\n\n#plots priors using the random variables\ndef plot_priors(variables, prior_dictionary = None):\n    if isinstance(variables[0], pm3.model.TransformedRV) == False and prior_dictionary is None:\n        raise Exception(\"prior dictionary should be provided. It can be generated by sample_prior_predictive\")\n    cols = 7\n    rows = int(math.ceil(len(variables)/cols))\n    fig, ax = plt.subplots(rows, cols, figsize=(15, 3*rows))\n    ax = np.reshape(ax, (-1, cols))\n    for i in range(rows):\n         for j in range(cols):\n            vi = i*cols + j\n            if vi &lt; len(variables):\n                var = variables[vi]\n                if isinstance(var, pm3.model.TransformedRV):\n                    sns.histplot(var.random(size=10000).flatten(), kde=True, ax=ax[i, j])\n                    #p.set_axis_labels(var.name)\n                    ax[i, j].set_title(var.name)\n                else:\n                    prior = prior_dictionary[var]\n                    sns.histplot(prior, kde=True, ax = ax[i, j])\n                    ax[i, j].set_title(var)\n    plt.tight_layout()\n    \n\nadstock_priors = [p for p in prior_names if p.startswith(\"car\")]\nplot_priors(adstock_priors, prior_pred)\nprint(f\"carryover priors: {len(adstock_priors)}\")\n\n# alpha_priors = [p for p in prior_names if p.startswith(\"sat\")]\n# plot_priors(alpha_priors, prior_pred)\n# print(f\"sat priors: {len(alpha_priors)}\")\n\nmedia_coef_priors = [p for p in prior_names if p.startswith(\"coef\")]\nplot_priors(media_coef_priors, prior_pred)\nprint(f\"coef priors: {len(media_coef_priors)}\")\n\ncontrol_coef_priors = [p for p in prior_names if p.startswith(\"control_\")] + [\"base\"]\nplot_priors(control_coef_priors, prior_pred)\nprint(f\"control coef priors: {len(control_coef_priors)}\")\n\n#plot_priors([\"sigma\"], prior_pred)\n\nprint(f\"sigma prior: 1\")\n\ncarryover priors: 7\ncoef priors: 7\ncontrol coef priors: 3\nsigma prior: 1"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#fit-the-model",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#fit-the-model",
    "title": "Bayesian MMM",
    "section": "Fit the model",
    "text": "Fit the model\n\nwith mmm1:\n  trace = pm3.sample(return_inferencedata=True, tune=3000, target_accept=0.95)\n  trace_summary = az.summary(trace)\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 01:16&lt;00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 01:15&lt;00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\n/usr/local/lib/python3.8/dist-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\ntrace_summary\n\n\n  \n    \n      \n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ncontrol_trend\n2.226\n2.019\n-1.785\n5.763\n0.048\n0.038\n1738.0\n1562.0\n1.0\n\n\ncontrol_season\n4.750\n1.860\n1.397\n8.340\n0.040\n0.028\n2176.0\n1508.0\n1.0\n\n\nbase\n6.811\n1.206\n4.585\n9.033\n0.030\n0.021\n1646.0\n1513.0\n1.0\n\n\ncoef_spend_channel_1\n0.915\n0.777\n0.005\n2.268\n0.017\n0.012\n1317.0\n833.0\n1.0\n\n\ncar_spend_channel_1\n0.446\n0.214\n0.063\n0.806\n0.004\n0.003\n2634.0\n1449.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\ncontribution_spend_channel_7[100]\n0.481\n0.323\n0.000\n1.034\n0.008\n0.006\n1404.0\n848.0\n1.0\n\n\ncontribution_spend_channel_7[101]\n0.635\n0.414\n0.000\n1.346\n0.010\n0.007\n1376.0\n767.0\n1.0\n\n\ncontribution_spend_channel_7[102]\n0.651\n0.424\n0.001\n1.382\n0.010\n0.007\n1388.0\n767.0\n1.0\n\n\ncontribution_spend_channel_7[103]\n0.701\n0.456\n0.000\n1.486\n0.011\n0.008\n1385.0\n767.0\n1.0\n\n\nnoise\n3.919\n0.306\n3.376\n4.498\n0.007\n0.005\n1978.0\n1310.0\n1.0\n\n\n\n\n746 rows Ã— 9 columns"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#posterior-distributions",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#posterior-distributions",
    "title": "Bayesian MMM",
    "section": "Posterior distributions",
    "text": "Posterior distributions\n\naz.plot_posterior(\n    trace,\n    var_names=['~contribution'],\n    filter_vars='like'\n)\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabda7341f0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabda6135e0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabda3fd820&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabda5197f0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabda2ba340&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd96bab20&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9662e50&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd96eee20&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9090a60&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd8ffd820&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd7e2bf70&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9dd2a90&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9ccf670&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9cef880&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9c244f0&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9d9f4f0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabd9e3adf0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fabdab3aa90&gt;]],\n      dtype=object)"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#predictions-vs-observations",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#predictions-vs-observations",
    "title": "Bayesian MMM",
    "section": "Predictions vs Observations",
    "text": "Predictions vs Observations\n\nwith mmm1:\n    posterior = pm3.sample_posterior_predictive(trace)\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:28&lt;00:00]\n    \n    \n\n\n\ny_pred = posterior['sales'].mean(0)*10000\ny_stds = posterior['sales'].std(0)*10000\n\nMAE = mean_absolute_error(y.values, y_pred)\nMAPE = mean_absolute_percentage_error(y.values, y_pred)*100\nSkillStr = 'MAE: %5d\\nMAPE: %5.2f%%'%(MAE,MAPE)\nfig, ax = plt.subplots(figsize=(12, 6))\nplt.subplots_adjust(left=0.15,\n                        bottom=0.15,\n                        right=0.95,\n                        top=0.9)\nax.plot(y.values, linewidth=2, c='r', label='Observations')\nax.plot(y_pred, linewidth=1, c='b', label='Mean prediction')\nax.fill_between(np.arange(len(y)), y_pred - 2*y_stds, y_pred + 2*y_stds, alpha=0.33)\nax.text(0.85,0.9,SkillStr, transform=ax.transAxes)\nax.legend(loc='upper center')\nax.set_xlabel('Week')\nax.set_ylabel('Revenue')\nplt.show()"
  },
  {
    "objectID": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#channel-contributions-and-roi",
    "href": "saved4later/bayesian_mmm_example_enhanced_newdataset.html#channel-contributions-and-roi",
    "title": "Bayesian MMM",
    "section": "Channel Contributions and ROI",
    "text": "Channel Contributions and ROI\n\ndef compute_mean(trace, channel):\n    return (trace\n            .posterior[f'{channel}']\n            .values\n            .reshape(2000, 104)\n            .mean(0)\n           )\n\nchannels = [f'contribution_spend_channel_{i}' for i in range(1,8)]\n\nunadj_contributions = pd.DataFrame(\n    {'Base+Trend+Seas': trace.posterior['base'].values.mean()\n                 +trace.posterior['control_trend'].values.mean()\n                 +trace.posterior['control_season'].values.mean()},\n    index=X.index\n)\n\nfor channel in channels:\n    unadj_contributions[channel] = compute_mean(trace, channel)\n\nadj_contributions = (unadj_contributions\n                     .div(unadj_contributions.sum(axis=1), axis=0)\n                     .mul(y, axis=0)\n                    )\n\nax = (adj_contributions\n      .plot.area(\n          figsize=(12, 6),\n          linewidth=1,\n          title='Predicted Revenue and Breakdown',\n          ylabel='Revenue',\n          xlabel='Week'\n      )\n     )\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(\n    handles[::-1], labels[::-1],\n    title='Channels', loc=\"upper right\",\n    #bbox_to_anchor=(1.01, 0.5)\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n#Calculate ROI for each channel\ntotal_contr = adj_contributions.sum(axis=0)\ntotal_spend = X.sum(axis=0)\n\nCchannels = [f'contribution_spend_channel_{i}' for i in range(1,8)]\n            \nSchannels = [f'spend_channel_{i}' for i in range(1,8)]\n\nROI_l= [None] * 7\nspend_l = [None] * 7\ncontr_l = [None] * 7\nfor i in range(7):\n    spend_l[i] = total_spend[Schannels[i]]\n    contr_l[i] = total_contr[Cchannels[i]]\n    ROI_l[i] = (contr_l[i] - spend_l[i])/spend_l[i] *100\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\nax1.bar(np.arange(1,8) - 0.2, spend_l, color = 'r', width = 0.4, label='Spend')\nax1.bar(np.arange(1,8) + 0.2, contr_l, color = 'b', width = 0.4, label='Contr')\nax1.set_xlabel('Marketing Channel')\nax1.set_ylabel('Contribution and Spends')\nax1.legend(loc='upper left')\n\nax2.bar(range(1,8),ROI_l)\nax2.set_xlabel('Marketing Channel')\nax2.set_ylabel('ROI (%)')\n\nplt.show()"
  }
]